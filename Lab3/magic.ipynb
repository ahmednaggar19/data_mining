{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification\n",
    "\n",
    "This dataset is generated to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. The dataset consists of two classes; gammas (signal) and hadrons (background). There\n",
    "are 12332 gamma events and 6688 hadron events. You are required to apply preprocessing techniques on this\n",
    "dataset and use the preprocessed dataset to construct different classification models such as Decision Trees,\n",
    "Na√Øve Bayes Classifier, Random Forests, AdaBoost, K-Nearest Neighbor (K-NN) and Support Vector\n",
    "Machines (SVM). You are also required to tune the parameters of these models, compare the performance of\n",
    "the learned models before and after preprocessing and compare the performance of models with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "data = pd.read_csv(\"dataset/magic04.data\",\n",
    "                  names = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\",\n",
    "                           \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.6240</td>\n",
       "      <td>21.1502</td>\n",
       "      <td>2.9085</td>\n",
       "      <td>0.2420</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>50.8761</td>\n",
       "      <td>43.1887</td>\n",
       "      <td>9.8145</td>\n",
       "      <td>3.6130</td>\n",
       "      <td>238.0980</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48.2468</td>\n",
       "      <td>17.3565</td>\n",
       "      <td>3.0332</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>8.5730</td>\n",
       "      <td>38.0957</td>\n",
       "      <td>10.5868</td>\n",
       "      <td>4.7920</td>\n",
       "      <td>219.0870</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.7897</td>\n",
       "      <td>13.7595</td>\n",
       "      <td>2.5521</td>\n",
       "      <td>0.4236</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>29.6339</td>\n",
       "      <td>20.4560</td>\n",
       "      <td>-2.9292</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>237.1340</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>96.2327</td>\n",
       "      <td>46.5165</td>\n",
       "      <td>4.1540</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>110.3550</td>\n",
       "      <td>85.0486</td>\n",
       "      <td>43.1844</td>\n",
       "      <td>4.8540</td>\n",
       "      <td>248.2260</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46.7619</td>\n",
       "      <td>15.1993</td>\n",
       "      <td>2.5786</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>24.7548</td>\n",
       "      <td>43.8771</td>\n",
       "      <td>-6.6812</td>\n",
       "      <td>7.8750</td>\n",
       "      <td>102.2510</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM3Long  fM3Trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "5   51.6240   21.1502  2.9085  0.2420  0.1340   50.8761  43.1887    9.8145   \n",
       "6   48.2468   17.3565  3.0332  0.2529  0.1515    8.5730  38.0957   10.5868   \n",
       "7   26.7897   13.7595  2.5521  0.4236  0.2174   29.6339  20.4560   -2.9292   \n",
       "8   96.2327   46.5165  4.1540  0.0779  0.0390  110.3550  85.0486   43.1844   \n",
       "9   46.7619   15.1993  2.5786  0.3377  0.1913   24.7548  43.8771   -6.6812   \n",
       "\n",
       "    fAlpha     fDist class  \n",
       "0  40.0920   81.8828     g  \n",
       "1   6.3609  205.2610     g  \n",
       "2  76.9600  256.7880     g  \n",
       "3  10.4490  116.7370     g  \n",
       "4   4.6480  356.4620     g  \n",
       "5   3.6130  238.0980     g  \n",
       "6   4.7920  219.0870     g  \n",
       "7   0.8120  237.1340     g  \n",
       "8   4.8540  248.2260     g  \n",
       "9   7.8750  102.2510     g  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19020\n"
     ]
    }
   ],
   "source": [
    "display(data.head(10))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data balancing\n",
    "We can notice that there is an imbalance between the number of rows existent in 'g' and the number of rows existent in 'h'.\n",
    "This problem must be solved by removing random samples from data where class is 'g' until both number of rows are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12332\n",
      "6688\n"
     ]
    }
   ],
   "source": [
    "data_g = data[data['class'] == 'g']\n",
    "data_h = data[data['class'] == 'h']\n",
    "\n",
    "print (len(data_g))\n",
    "print (len(data_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_g = data_g.sample(n=len(data_h))\n",
    "\n",
    "data = pd.concat([data_g, data_h], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6688\n"
     ]
    }
   ],
   "source": [
    "print(len(data[data['class'] == 'g']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Using describe method to check each attribute statistics, we can observe different ranges of values for each attribue. Therefore, normalization should be used to prevent features with large domain of values to dominate other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "      <td>13376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>57.260836</td>\n",
       "      <td>23.698595</td>\n",
       "      <td>2.841548</td>\n",
       "      <td>0.379614</td>\n",
       "      <td>0.214959</td>\n",
       "      <td>-7.203546</td>\n",
       "      <td>7.387873</td>\n",
       "      <td>0.349263</td>\n",
       "      <td>31.491950</td>\n",
       "      <td>194.875838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47.038755</td>\n",
       "      <td>20.943337</td>\n",
       "      <td>0.477324</td>\n",
       "      <td>0.185325</td>\n",
       "      <td>0.112541</td>\n",
       "      <td>65.469857</td>\n",
       "      <td>56.422424</td>\n",
       "      <td>23.259910</td>\n",
       "      <td>27.032611</td>\n",
       "      <td>76.523942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.283500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.941300</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-457.916100</td>\n",
       "      <td>-331.780000</td>\n",
       "      <td>-205.894700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.670300</td>\n",
       "      <td>11.693425</td>\n",
       "      <td>2.494200</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.126775</td>\n",
       "      <td>-23.861250</td>\n",
       "      <td>-14.936650</td>\n",
       "      <td>-10.935350</td>\n",
       "      <td>7.275975</td>\n",
       "      <td>142.057050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.663450</td>\n",
       "      <td>17.138050</td>\n",
       "      <td>2.752600</td>\n",
       "      <td>0.355800</td>\n",
       "      <td>0.197450</td>\n",
       "      <td>2.885500</td>\n",
       "      <td>14.497950</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>23.479100</td>\n",
       "      <td>192.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>76.009700</td>\n",
       "      <td>26.289150</td>\n",
       "      <td>3.119125</td>\n",
       "      <td>0.506825</td>\n",
       "      <td>0.288000</td>\n",
       "      <td>23.805625</td>\n",
       "      <td>34.510300</td>\n",
       "      <td>11.252350</td>\n",
       "      <td>52.890475</td>\n",
       "      <td>242.441625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>334.177000</td>\n",
       "      <td>256.382000</td>\n",
       "      <td>5.323300</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>575.240700</td>\n",
       "      <td>238.321000</td>\n",
       "      <td>179.851000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>495.561000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            fLength        fWidth         fSize         fConc        fConc1  \\\n",
       "count  13376.000000  13376.000000  13376.000000  13376.000000  13376.000000   \n",
       "mean      57.260836     23.698595      2.841548      0.379614      0.214959   \n",
       "std       47.038755     20.943337      0.477324      0.185325      0.112541   \n",
       "min        4.283500      0.000000      1.941300      0.013100      0.000300   \n",
       "25%       24.670300     11.693425      2.494200      0.232900      0.126775   \n",
       "50%       38.663450     17.138050      2.752600      0.355800      0.197450   \n",
       "75%       76.009700     26.289150      3.119125      0.506825      0.288000   \n",
       "max      334.177000    256.382000      5.323300      0.893000      0.643000   \n",
       "\n",
       "              fAsym       fM3Long      fM3Trans        fAlpha         fDist  \n",
       "count  13376.000000  13376.000000  13376.000000  13376.000000  13376.000000  \n",
       "mean      -7.203546      7.387873      0.349263     31.491950    194.875838  \n",
       "std       65.469857     56.422424     23.259910     27.032611     76.523942  \n",
       "min     -457.916100   -331.780000   -205.894700      0.000000      1.282600  \n",
       "25%      -23.861250    -14.936650    -10.935350      7.275975    142.057050  \n",
       "50%        2.885500     14.497950      0.706600     23.479100    192.866500  \n",
       "75%       23.805625     34.510300     11.252350     52.890475    242.441625  \n",
       "max      575.240700    238.321000    179.851000     90.000000    495.561000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the correlation matrix, that there is high correlation among fLength, fWidth, and fSize features. Another slight correlation between fDist feature and the first 3 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f586f559390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data.corr())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots can assert our previous claim of features have different ranges of values which may affect the learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = data.columns\n",
    "box_plots_1 = data.boxplot(column = ['fLength', 'fWidth','fSize','fConc', 'fConc1'], patch_artist = True, showfliers=False)\n",
    "plt.figure()\n",
    "box_plots_2 = data.boxplot(column = ['fAsym','fM3Long','fM3Trans', 'fAlpha', 'fDist'], patch_artist = True, showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "As a result of our conclusions above, it is obvious now that we need to perform 2 important preprocessing steps, features normalization and features selection.\n",
    "We are going to use z-normalization to normalize the data. We are not going use min-max since there are some alot of outliers in each feature. \n",
    "We will also apply PCA to reduce the correlated, unnecessary features in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first normalize the data using L2 Norm, we use Z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247350</td>\n",
       "      <td>0.061871</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.211877</td>\n",
       "      <td>0.227675</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>0.915067</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335547</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.207229</td>\n",
       "      <td>0.076424</td>\n",
       "      <td>-0.228134</td>\n",
       "      <td>0.539680</td>\n",
       "      <td>0.631681</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.226525</td>\n",
       "      <td>0.090560</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>-0.366666</td>\n",
       "      <td>0.103166</td>\n",
       "      <td>0.073526</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.888763</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.311248</td>\n",
       "      <td>0.099865</td>\n",
       "      <td>0.012521</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.215388</td>\n",
       "      <td>0.331985</td>\n",
       "      <td>-0.071522</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>0.855071</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.202091</td>\n",
       "      <td>0.110648</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.228396</td>\n",
       "      <td>0.153716</td>\n",
       "      <td>0.088794</td>\n",
       "      <td>0.144489</td>\n",
       "      <td>0.917658</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.239334</td>\n",
       "      <td>0.051951</td>\n",
       "      <td>0.014617</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>0.185718</td>\n",
       "      <td>-0.021984</td>\n",
       "      <td>0.077515</td>\n",
       "      <td>0.944673</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.113230</td>\n",
       "      <td>0.056469</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>-0.112341</td>\n",
       "      <td>-0.101249</td>\n",
       "      <td>-0.055949</td>\n",
       "      <td>0.030801</td>\n",
       "      <td>0.978212</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.172902</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>-0.205675</td>\n",
       "      <td>-0.122159</td>\n",
       "      <td>-0.086258</td>\n",
       "      <td>0.039869</td>\n",
       "      <td>0.944739</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.196607</td>\n",
       "      <td>0.045592</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.054225</td>\n",
       "      <td>0.119092</td>\n",
       "      <td>-0.031436</td>\n",
       "      <td>0.016489</td>\n",
       "      <td>0.969936</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.333121</td>\n",
       "      <td>0.091996</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.105151</td>\n",
       "      <td>0.208461</td>\n",
       "      <td>0.074981</td>\n",
       "      <td>0.065870</td>\n",
       "      <td>0.903260</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth     fSize     fConc    fConc1     fAsym   fM3Long  \\\n",
       "0  0.247350  0.061871  0.011812  0.000845  0.000501  0.211877  0.227675   \n",
       "1  0.335547  0.308300  0.035320  0.002433  0.001300  0.207229  0.076424   \n",
       "2  0.226525  0.090560  0.008681  0.000414  0.000215 -0.366666  0.103166   \n",
       "3  0.311248  0.099865  0.012521  0.000579  0.000306  0.215388  0.331985   \n",
       "4  0.202091  0.110648  0.016118  0.002101  0.001138  0.228396  0.153716   \n",
       "5  0.239334  0.051951  0.014617  0.002362  0.001614  0.080048  0.185718   \n",
       "6  0.113230  0.056469  0.011368  0.002937  0.001676 -0.112341 -0.101249   \n",
       "7  0.172902  0.104400  0.019836  0.004342  0.002411 -0.205675 -0.122159   \n",
       "8  0.196607  0.045592  0.010200  0.001693  0.000856  0.054225  0.119092   \n",
       "9  0.333121  0.091996  0.014505  0.001808  0.001021  0.105151  0.208461   \n",
       "\n",
       "   fM3Trans    fAlpha     fDist class  \n",
       "0  0.027397  0.004909  0.915067     g  \n",
       "1 -0.228134  0.539680  0.631681     g  \n",
       "2  0.073526  0.004228  0.888763     g  \n",
       "3 -0.071522  0.011250  0.855071     g  \n",
       "4  0.088794  0.144489  0.917658     g  \n",
       "5 -0.021984  0.077515  0.944673     g  \n",
       "6 -0.055949  0.030801  0.978212     g  \n",
       "7 -0.086258  0.039869  0.944739     g  \n",
       "8 -0.031436  0.016489  0.969936     g  \n",
       "9  0.074981  0.065870  0.903260     g  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = data.drop(columns=['class'])\n",
    "Y = pd.DataFrame(data['class'], columns=['class'])\n",
    "X = pd.DataFrame(normalize(X), columns=X.columns)\n",
    "\n",
    "data_norm = pd.concat([X, Y], axis=1)\n",
    "\n",
    "data_norm.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply PCA to the normalized data with number of components = 5, we choose 5 based on our visualization results\n",
    "that show that the number of features can be reduced to 5 uncorrelated unique features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.337600e+04</td>\n",
       "      <td>1.337600e+04</td>\n",
       "      <td>1.337600e+04</td>\n",
       "      <td>1.337600e+04</td>\n",
       "      <td>1.337600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.499315e-18</td>\n",
       "      <td>-3.824692e-17</td>\n",
       "      <td>-1.779544e-17</td>\n",
       "      <td>-1.593622e-17</td>\n",
       "      <td>-4.316058e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.414170e-01</td>\n",
       "      <td>1.915896e-01</td>\n",
       "      <td>1.713429e-01</td>\n",
       "      <td>1.491502e-01</td>\n",
       "      <td>8.377241e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.887379e-01</td>\n",
       "      <td>-2.779743e-01</td>\n",
       "      <td>-4.158323e-01</td>\n",
       "      <td>-4.627462e-01</td>\n",
       "      <td>-4.355954e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.648228e-01</td>\n",
       "      <td>-1.373259e-01</td>\n",
       "      <td>-1.185146e-01</td>\n",
       "      <td>-9.736965e-02</td>\n",
       "      <td>-5.669858e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.435430e-02</td>\n",
       "      <td>-4.578463e-02</td>\n",
       "      <td>-3.085270e-02</td>\n",
       "      <td>-4.676845e-03</td>\n",
       "      <td>1.379971e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.134313e-01</td>\n",
       "      <td>7.868478e-02</td>\n",
       "      <td>9.212340e-02</td>\n",
       "      <td>8.066759e-02</td>\n",
       "      <td>5.584991e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.048329e+00</td>\n",
       "      <td>9.899469e-01</td>\n",
       "      <td>8.401563e-01</td>\n",
       "      <td>8.764813e-01</td>\n",
       "      <td>4.810616e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pca_0         pca_1         pca_2         pca_3         pca_4\n",
       "count  1.337600e+04  1.337600e+04  1.337600e+04  1.337600e+04  1.337600e+04\n",
       "mean  -8.499315e-18 -3.824692e-17 -1.779544e-17 -1.593622e-17 -4.316058e-19\n",
       "std    2.414170e-01  1.915896e-01  1.713429e-01  1.491502e-01  8.377241e-02\n",
       "min   -4.887379e-01 -2.779743e-01 -4.158323e-01 -4.627462e-01 -4.355954e-01\n",
       "25%   -1.648228e-01 -1.373259e-01 -1.185146e-01 -9.736965e-02 -5.669858e-02\n",
       "50%   -4.435430e-02 -4.578463e-02 -3.085270e-02 -4.676845e-03  1.379971e-03\n",
       "75%    1.134313e-01  7.868478e-02  9.212340e-02  8.066759e-02  5.584991e-02\n",
       "max    1.048329e+00  9.899469e-01  8.401563e-01  8.764813e-01  4.810616e-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(data_norm.loc[:, data_norm.columns != 'class'])\n",
    "columns = ['pca_%i' % i for i in range(5)]\n",
    "X_pca = pd.DataFrame(pca.transform(data_norm.loc[:, data_norm.columns != 'class']),\n",
    "                         columns=columns, index=data_norm.index)\n",
    "\n",
    "data_proc = pd.concat([X_pca, Y], axis=1)\n",
    "display(data_proc.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "We need to randomly split the data to 70% training set and 30% testing set. We do this by first separating between class column (y) from features (x), and finally use sklearn. We use two versions, one for the unpreprocessed data and one for the preprocessed one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.3)\n",
    "train_proc_df, test_proc_df = train_test_split(data_proc, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9363\n",
      "4013\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features(train_df, test_df):\n",
    "    X_train = train_df.drop('class', axis=1)\n",
    "    y_train = train_df['class']\n",
    "\n",
    "    X_test = test_df.drop('class', axis=1)\n",
    "    y_test = test_df['class']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_features(train_df, test_df)\n",
    "X_proc_train, y_proc_train, X_proc_test, y_proc_test = split_features(train_proc_df, test_proc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def model_scores(y_pred, y_test):\n",
    "    print(\"Precision Score : \" + str(precision_score(y_test, y_pred, average='macro')))\n",
    "    print(\"Recall Score : \" + str(recall_score(y_test, y_pred, average='macro')))\n",
    "    print(\"F Score : \" + str(f1_score(y_test, y_pred, average=\"macro\")))\n",
    "    print(\"Confusion matrix : \" + str(confusion_matrix(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. AdaBoost Classifier\n",
    "We can see from the following results that the AdaBoostClassifier model achieves a maximum accuracy result of **82.63%** at number of estimators = 1000, using default base estimator as DecisionTreeClassifier(max_depth=1) *on . It can be noticed that the increase of the number of estimators results in a better accuracy, but takes a longer time to fit and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7699975080986793  at n_estimators =  10  time =  0.23298239707946777\n",
      "0.8203339147769748  at n_estimators =  120  time =  2.7992496490478516\n",
      "0.8208322950411163  at n_estimators =  230  time =  5.558085918426514\n",
      "0.8240717667580364  at n_estimators =  340  time =  8.814775943756104\n",
      "0.819087964116621  at n_estimators =  450  time =  13.28333330154419\n",
      "0.8220782457014703  at n_estimators =  560  time =  18.301065921783447\n",
      "0.8230750062297533  at n_estimators =  670  time =  21.133208990097046\n",
      "0.8218290555693994  at n_estimators =  780  time =  35.6477267742157\n",
      "0.8210814851731871  at n_estimators =  890  time =  29.04766845703125\n",
      "0.8208322950411163  at n_estimators =  1000  time =  28.00920557975769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "\n",
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = AdaBoostClassifier(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using processed data (after normalization and feature reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6902566658360328  at n_estimators =  10  time =  0.300262451171875\n",
      "0.7076999750809868  at n_estimators =  120  time =  2.4625580310821533\n",
      "0.7079491652130575  at n_estimators =  230  time =  4.472189426422119\n",
      "0.7072015948168453  at n_estimators =  340  time =  7.2161760330200195\n",
      "0.706454024420633  at n_estimators =  450  time =  8.430338382720947\n",
      "0.7039621230999252  at n_estimators =  560  time =  12.976925373077393\n",
      "0.7072015948168453  at n_estimators =  670  time =  14.420040369033813\n",
      "0.7081983553451283  at n_estimators =  780  time =  15.408960819244385\n",
      "0.7076999750809868  at n_estimators =  890  time =  16.96372699737549\n",
      "0.7081983553451283  at n_estimators =  1000  time =  18.296377182006836\n"
     ]
    }
   ],
   "source": [
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = AdaBoostClassifier(n_estimators=n)\n",
    "    model.fit(X_proc_train, y_proc_train)\n",
    "    y_pred = model.predict(X_proc_test)\n",
    "    print(accuracy_score(y_pred, y_proc_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the max depth of the decision tree classifier acting as a base estimator for the adaboost algorithm, we notice that the accuracy has increased to **84.77%** at n_estimators=890, but the time taken to fit the model is significantly increasing too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8240717667580364  at n_estimators =  10  time =  0.7072975635528564\n",
      "0.8128582108148518  at n_estimators =  120  time =  8.391402244567871\n",
      "0.8146025417393471  at n_estimators =  230  time =  16.007941961288452\n",
      "0.8268128582108148  at n_estimators =  340  time =  22.87144660949707\n",
      "0.8283079990032395  at n_estimators =  450  time =  30.063548803329468\n",
      "0.8347869424370795  at n_estimators =  560  time =  38.66458034515381\n",
      "0.8425118365312734  at n_estimators =  670  time =  48.35356950759888\n",
      "0.8372788437577872  at n_estimators =  780  time =  53.80435252189636\n",
      "0.8402691253426364  at n_estimators =  890  time =  62.19586515426636\n",
      "0.845003737851981  at n_estimators =  1000  time =  81.35519528388977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "\n",
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. K-NN classifier\n",
    "We can see from the following results that the best accuracy of **76.7%** is achieved at k = 24. Which is less than the accuracy we achieved using **AdaBoost (84.77%)**, but it is noticed that the time taken to fit the model and predict is way less than that of AdaBoost (0.17s vs 57s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7645153251931224  at k =  5  time =  0.5589761734008789\n",
      "0.7607774732120608  at k =  8  time =  0.1688859462738037\n",
      "0.7647645153251931  at k =  12  time =  0.14991092681884766\n",
      "0.7642661350610516  at k =  16  time =  0.16152620315551758\n",
      "0.76052828307999  at k =  20  time =  0.22388243675231934\n",
      "0.7617742337403439  at k =  24  time =  0.3950991630554199\n",
      "0.7600299028158485  at k =  28  time =  0.2769935131072998\n",
      "0.7607774732120608  at k =  32  time =  0.21049904823303223\n",
      "0.7587839521554947  at k =  36  time =  0.23679757118225098\n",
      "0.75728881136307  at k =  40  time =  0.6356022357940674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_s = [int(x) for x in np.linspace(start=5, stop=40, num=10)]\n",
    "\n",
    "for k in k_s:\n",
    "    start_time = time()\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at k = \", k, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7199102915524546  at k =  5  time =  0.0769658088684082\n",
      "0.7136805382506852  at k =  8  time =  0.06995916366577148\n",
      "0.7209070520807376  at k =  12  time =  0.07504987716674805\n",
      "0.7204086718165961  at k =  16  time =  0.09013223648071289\n",
      "0.72489409419387  at k =  20  time =  0.0945577621459961\n",
      "0.7268876152504361  at k =  24  time =  0.09641265869140625\n",
      "0.7288811363070022  at k =  28  time =  0.12529206275939941\n",
      "0.7296287067032146  at k =  32  time =  0.11247134208679199\n",
      "0.732369798155993  at k =  36  time =  0.14387774467468262\n",
      "0.7326189882880638  at k =  40  time =  0.12660765647888184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_s = [int(x) for x in np.linspace(start=5, stop=40, num=10)]\n",
    "\n",
    "for k in k_s:\n",
    "    start_time = time()\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_proc_train, y_proc_train)\n",
    "    y_pred = model.predict(X_proc_test)\n",
    "    print(accuracy_score(y_pred, y_proc_test), \" at k = \", k, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forests\n",
    "The random forests is trained the exact same way AdaBoost is trained earlier. It can be seen that random forests achieves maximum accuracy of **86.51%** at n_estimators = 670, and takes 18.8s to fit and predict data, which is better than AdaBoost in terms of both accuracy (82.6%) and time (21s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8360328930974333  at n_estimators =  10  time =  0.31835484504699707\n",
      "0.852728631946175  at n_estimators =  120  time =  3.703810691833496\n",
      "0.8519810615499627  at n_estimators =  230  time =  6.618767023086548\n",
      "0.8539745826065288  at n_estimators =  340  time =  13.050545454025269\n",
      "0.853725392474458  at n_estimators =  450  time =  14.580679655075073\n",
      "0.8547221530027411  at n_estimators =  560  time =  22.304248332977295\n",
      "0.8529778220782457  at n_estimators =  670  time =  31.998010396957397\n",
      "0.8544729628706703  at n_estimators =  780  time =  40.683979749679565\n",
      "0.8552205332668826  at n_estimators =  890  time =  31.256914377212524\n",
      "0.8544729628706703  at n_estimators =  1000  time =  35.4376962184906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "\n",
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = RandomForestClassifier(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7174183902317468  at n_estimators =  10  time =  0.2803490161895752\n",
      "0.7463244455519561  at n_estimators =  120  time =  3.616668462753296\n",
      "0.7493147271368054  at n_estimators =  230  time =  8.564835548400879\n",
      "0.74433092449539  at n_estimators =  340  time =  9.31927752494812\n",
      "0.7465736356840269  at n_estimators =  450  time =  10.936365127563477\n",
      "0.7445801146274608  at n_estimators =  560  time =  13.834892272949219\n",
      "0.7453276850236731  at n_estimators =  670  time =  18.890822410583496\n",
      "0.7440817343633193  at n_estimators =  780  time =  18.567312479019165\n",
      "0.7455768751557438  at n_estimators =  890  time =  23.345545053482056\n",
      "0.7455768751557438  at n_estimators =  1000  time =  28.84114146232605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "\n",
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = RandomForestClassifier(n_estimators=n)\n",
    "    model.fit(X_proc_train, y_proc_train)\n",
    "    y_pred = model.predict(X_proc_test)\n",
    "    print(accuracy_score(y_pred, y_proc_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. SVM Classifier\n",
    "The Linear SVM classifier performed badly with accuracy less than **57%**. Which means that the data was not linearly separable. The wide range of C regularization values did not introduce big contribution to the accuracy, which endorses our claim.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5033640667829554  at C =  1  time =  8.371149778366089\n",
      "Precision Score : 0.5060828483498141\n",
      "Recall Score : 0.5608074888883371\n",
      "F Score : 0.3614191538907127\n",
      "Confusion matrix : [[1956 1954]\n",
      " [  39   64]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5026164963867431  at C =  5  time =  10.108948945999146\n",
      "Precision Score : 0.5053166861653638\n",
      "Recall Score : 0.5473496411917214\n",
      "F Score : 0.36295143256530704\n",
      "Confusion matrix : [[1948 1949]\n",
      " [  47   69]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5026164963867431  at C =  10  time =  10.118077039718628\n",
      "Precision Score : 0.5053166861653638\n",
      "Recall Score : 0.5473496411917214\n",
      "F Score : 0.36295143256530704\n",
      "Confusion matrix : [[1948 1949]\n",
      " [  47   69]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5026164963867431  at C =  100  time =  11.120841264724731\n",
      "Precision Score : 0.5053166861653638\n",
      "Recall Score : 0.5473496411917214\n",
      "F Score : 0.36295143256530704\n",
      "Confusion matrix : [[1948 1949]\n",
      " [  47   69]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5026164963867431  at C =  1000  time =  10.667268991470337\n",
      "Precision Score : 0.5053166861653638\n",
      "Recall Score : 0.5473496411917214\n",
      "F Score : 0.36295143256530704\n",
      "Confusion matrix : [[1948 1949]\n",
      " [  47   69]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "C = [1, 5, 10, 100, 1000]\n",
    "\n",
    "for c in C:\n",
    "    start_time = time()\n",
    "    model = svm.SVC(C=c)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at C = \", c, \" time = \", time() - start_time)\n",
    "    model_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7136805382506852  at C =  1  time =  4.080479860305786\n",
      "Precision Score : 0.7136897727346113\n",
      "Recall Score : 0.71486667049618\n",
      "F Score : 0.7132905719436908\n",
      "Confusion matrix : [[1506  649]\n",
      " [ 500 1358]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7224021928731622  at C =  5  time =  3.9426538944244385\n",
      "Precision Score : 0.7224031691671373\n",
      "Recall Score : 0.722416427832268\n",
      "F Score : 0.7223983143508199\n",
      "Confusion matrix : [[1457  565]\n",
      " [ 549 1442]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7221530027410914  at C =  10  time =  5.313428163528442\n",
      "Precision Score : 0.7221515572862875\n",
      "Recall Score : 0.722182020077059\n",
      "F Score : 0.7221430645943424\n",
      "Confusion matrix : [[1437  546]\n",
      " [ 569 1461]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7253924744580115  at C =  100  time =  10.932708740234375\n",
      "Precision Score : 0.7253868687907379\n",
      "Recall Score : 0.7258462979721503\n",
      "F Score : 0.7252511945908173\n",
      "Confusion matrix : [[1410  506]\n",
      " [ 596 1501]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "C = [1, 5, 10, 100, 1000]\n",
    "\n",
    "for c in C:\n",
    "    start_time = time()\n",
    "    model = svm.SVC(C=c)\n",
    "    model.fit(X_proc_train, y_proc_train)\n",
    "    y_pred = model.predict(X_proc_test)\n",
    "    print(accuracy_score(y_pred, y_proc_test), \" at C = \", c, \" time = \", time() - start_time)\n",
    "    model_scores(y_proc_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Naive Bayes Classifier\n",
    "Bayes Classifier behaved relatively better than SVMs but not as good as AdaBoost. The accuracy was **65%** which indicates that the features are dependent on each other which violates the assumption made by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "start_time = time()\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test), \" using Naive Bayes Classifier \", \" time = \", time() - start_time)\n",
    "model_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "start_time = time()\n",
    "model = GaussianNB()\n",
    "model.fit(X_proc_train, y_proc_train)\n",
    "y_pred = model.predict(X_proc_test)\n",
    "print(accuracy_score(y_pred, y_proc_test), \" using Naive Bayes Classifier \", \" time = \", time() - start_time)\n",
    "model_scores(y_proc_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Decision Tree Classifier\n",
    "The Decision tree performed much better than previous classifiers. The inaccuracy could be justified by claiming that the decision tree only involves one feature during partitioning at each node. Accuracy was **79%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "start_time = time()\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test), \" using Decision Tree Classifier \", \" time = \", time() - start_time)\n",
    "model_scores(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
