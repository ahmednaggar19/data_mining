{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification\n",
    "\n",
    "This dataset is generated to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. The dataset consists of two classes; gammas (signal) and hadrons (background). There\n",
    "are 12332 gamma events and 6688 hadron events. You are required to apply preprocessing techniques on this\n",
    "dataset and use the preprocessed dataset to construct different classification models such as Decision Trees,\n",
    "Na√Øve Bayes Classifier, Random Forests, AdaBoost, K-Nearest Neighbor (K-NN) and Support Vector\n",
    "Machines (SVM). You are also required to tune the parameters of these models, compare the performance of\n",
    "the learned models before and after preprocessing and compare the performance of models with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "data = pd.read_csv(\"dataset/magic04.data\",\n",
    "                  names = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\",\n",
    "                           \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.6240</td>\n",
       "      <td>21.1502</td>\n",
       "      <td>2.9085</td>\n",
       "      <td>0.2420</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>50.8761</td>\n",
       "      <td>43.1887</td>\n",
       "      <td>9.8145</td>\n",
       "      <td>3.6130</td>\n",
       "      <td>238.0980</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48.2468</td>\n",
       "      <td>17.3565</td>\n",
       "      <td>3.0332</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>8.5730</td>\n",
       "      <td>38.0957</td>\n",
       "      <td>10.5868</td>\n",
       "      <td>4.7920</td>\n",
       "      <td>219.0870</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.7897</td>\n",
       "      <td>13.7595</td>\n",
       "      <td>2.5521</td>\n",
       "      <td>0.4236</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>29.6339</td>\n",
       "      <td>20.4560</td>\n",
       "      <td>-2.9292</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>237.1340</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>96.2327</td>\n",
       "      <td>46.5165</td>\n",
       "      <td>4.1540</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>110.3550</td>\n",
       "      <td>85.0486</td>\n",
       "      <td>43.1844</td>\n",
       "      <td>4.8540</td>\n",
       "      <td>248.2260</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46.7619</td>\n",
       "      <td>15.1993</td>\n",
       "      <td>2.5786</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>24.7548</td>\n",
       "      <td>43.8771</td>\n",
       "      <td>-6.6812</td>\n",
       "      <td>7.8750</td>\n",
       "      <td>102.2510</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM3Long  fM3Trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "5   51.6240   21.1502  2.9085  0.2420  0.1340   50.8761  43.1887    9.8145   \n",
       "6   48.2468   17.3565  3.0332  0.2529  0.1515    8.5730  38.0957   10.5868   \n",
       "7   26.7897   13.7595  2.5521  0.4236  0.2174   29.6339  20.4560   -2.9292   \n",
       "8   96.2327   46.5165  4.1540  0.0779  0.0390  110.3550  85.0486   43.1844   \n",
       "9   46.7619   15.1993  2.5786  0.3377  0.1913   24.7548  43.8771   -6.6812   \n",
       "\n",
       "    fAlpha     fDist class  \n",
       "0  40.0920   81.8828     g  \n",
       "1   6.3609  205.2610     g  \n",
       "2  76.9600  256.7880     g  \n",
       "3  10.4490  116.7370     g  \n",
       "4   4.6480  356.4620     g  \n",
       "5   3.6130  238.0980     g  \n",
       "6   4.7920  219.0870     g  \n",
       "7   0.8120  237.1340     g  \n",
       "8   4.8540  248.2260     g  \n",
       "9   7.8750  102.2510     g  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19020\n"
     ]
    }
   ],
   "source": [
    "display(data.head(10))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data balancing\n",
    "We can notice that there is an imbalance between the number of rows existent in 'g' and the number of rows existent in 'h'.\n",
    "This problem must be solved by removing random samples from data where class is 'g' until both number of rows are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12332\n",
      "6688\n"
     ]
    }
   ],
   "source": [
    "data_g = data[data['class'] == 'g']\n",
    "data_h = data[data['class'] == 'h']\n",
    "\n",
    "print (len(data_g))\n",
    "print (len(data_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_g = data_g.sample(n=len(data_h))\n",
    "\n",
    "data = pd.concat([data_g, data_h], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6688\n"
     ]
    }
   ],
   "source": [
    "print(len(data[data['class'] == 'g']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "We need to randomly split the data to 70% training set and 30% testing set. We do this by first separating between class column (y) from features (x), and finally use sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9363\n",
      "4013\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train = train_df.drop('class', axis=1)\n",
    "y_train = train_df['class']\n",
    "\n",
    "X_test = test_df.drop('class', axis=1)\n",
    "y_test = test_df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. AdaBoost Classifier\n",
    "We can see from the following results that the AdaBoostClassifier model achieves a maximum accuracy result of **82.63%** at number of estimators = 1000, using default base estimator as DecisionTreeClassifier(max_depth=1). It can be noticed that the increase of the number of estimators results in a better accuracy, but takes a longer time to fit and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7680039870421131  at n_estimators =  10  time =  0.2566192150115967\n",
      "0.8258160976825317  at n_estimators =  120  time =  2.842287540435791\n",
      "0.8250685272863194  at n_estimators =  230  time =  5.582809209823608\n",
      "0.8230750062297533  at n_estimators =  340  time =  8.04919719696045\n",
      "0.8238225766259656  at n_estimators =  450  time =  10.693397521972656\n",
      "0.8230750062297533  at n_estimators =  560  time =  13.367497444152832\n",
      "0.8230750062297533  at n_estimators =  670  time =  16.633725881576538\n",
      "0.8260652878146025  at n_estimators =  780  time =  21.740336418151855\n",
      "0.8253177174183902  at n_estimators =  890  time =  24.896631240844727\n",
      "0.8263144779466733  at n_estimators =  1000  time =  27.745393991470337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "\n",
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = AdaBoostClassifier(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the max depth of the decision tree classifier acting as a base estimator for the adaboost algorithm, we notice that the accuracy has increased to **84.77%** at n_estimators=890, but the time taken to fit the model is significantly increasing too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827809618739098  at n_estimators =  10  time =  0.6828384399414062\n",
      "0.8198355345128333  at n_estimators =  120  time =  7.991923570632935\n",
      "0.8280588088711687  at n_estimators =  230  time =  15.19045352935791\n",
      "0.8357837029653625  at n_estimators =  340  time =  23.63278603553772\n",
      "0.8317966608522303  at n_estimators =  450  time =  31.768905639648438\n",
      "0.8407675056067779  at n_estimators =  560  time =  43.15023469924927\n",
      "0.8357837029653625  at n_estimators =  670  time =  48.17034864425659\n",
      "0.8387739845502118  at n_estimators =  780  time =  50.58229374885559\n",
      "0.8477448293047596  at n_estimators =  890  time =  57.874876499176025\n",
      "0.847246449040618  at n_estimators =  1000  time =  64.2132306098938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "\n",
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. K-NN classifier\n",
    "We can see from the following results that the best accuracy of **76.7%** is achieved at k = 24. Which is less than the accuracy we achieved using **AdaBoost (84.77%)**, but it is noticed that the time taken to fit the model and predict is way less than that of AdaBoost (0.17s vs 57s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7622726140044854  at k =  5  time =  0.14264750480651855\n",
      "0.7600299028158485  at k =  8  time =  0.13023829460144043\n",
      "0.75728881136307  at k =  12  time =  0.15084028244018555\n",
      "0.7600299028158485  at k =  16  time =  0.15511250495910645\n",
      "0.7630201844006977  at k =  20  time =  0.18316936492919922\n",
      "0.76700722651383  at k =  24  time =  0.17857074737548828\n",
      "0.7647645153251931  at k =  28  time =  0.2509725093841553\n",
      "0.7667580363817593  at k =  32  time =  0.1994931697845459\n",
      "0.7617742337403439  at k =  36  time =  0.2338404655456543\n",
      "0.7625218041365562  at k =  40  time =  0.24196386337280273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_s = [int(x) for x in np.linspace(start=5, stop=40, num=10)]\n",
    "\n",
    "for k in k_s:\n",
    "    start_time = time()\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at k = \", k, \" time = \", time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forests\n",
    "The random forests is trained the exact same way AdaBoost is trained earlier. It can be seen that random forests achieves maximum accuracy of **86.51%** at n_estimators = 670, and takes 18.8s to fit and predict data, which is better than AdaBoost in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8479940194368303  at n_estimators =  10  time =  0.3142087459564209\n",
      "0.8631946174931473  at n_estimators =  120  time =  3.436056137084961\n",
      "0.8649389484176426  at n_estimators =  230  time =  6.41419529914856\n",
      "0.8616994767007227  at n_estimators =  340  time =  9.746688604354858\n",
      "0.8629454273610765  at n_estimators =  450  time =  12.339335203170776\n",
      "0.8612010964365812  at n_estimators =  560  time =  15.854724168777466\n",
      "0.8651881385497134  at n_estimators =  670  time =  18.82325768470764\n",
      "0.8644405681535011  at n_estimators =  780  time =  26.190967321395874\n",
      "0.862447047096935  at n_estimators =  890  time =  27.244975566864014\n",
      "0.8649389484176426  at n_estimators =  1000  time =  30.12388300895691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "\n",
    "for n in n_estimators:\n",
    "    start_time = time()\n",
    "    model = RandomForestClassifier(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(accuracy_score(y_pred, y_test), \" at n_estimators = \", n, \" time = \", time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
